{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 2.Tagging\n",
    "#### 2.1 LSTM Tagging"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 2.1.1 Read dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "with open(\"./Mydata.json\",'r+') as f:\n",
    "    Mydata = json.load(f)\n",
    "tmp = {}\n",
    "for key in Mydata['id2label']:\n",
    "    tmp[int(key)] = Mydata['id2label'][key]\n",
    "Mydata['id2label'] = tmp\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 2.2.2 Generate DataLoader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data: list, label, word2id, id2label, label2id, device):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.word2id = word2id\n",
    "        self.id2label = id2label\n",
    "        self.label2id = label2id\n",
    "        self.device = device\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        data = []\n",
    "        label = []\n",
    "        # print(item)\n",
    "        for i in range(len(self.data[item])):\n",
    "            data.append(self.word2id[self.data[item][i]])\n",
    "            label.append(self.label2id[self.label[item][i]])\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def batchfy(self, batchData):\n",
    "        # print(batchData)\n",
    "        maxlen = max([len(each[0]) for each in batchData])\n",
    "        padc, padi = self.word2id[\"PAD\"], self.label2id[\"PAD\"]\n",
    "        batchDatas = []\n",
    "        padlabels = []\n",
    "        attention = []\n",
    "        for i in range(len(batchData)):\n",
    "            if len(batchData[i][0]) >= maxlen:\n",
    "                batchDatas.append(batchData[i][0][:])\n",
    "                padlabels.append(batchData[i][1][:])\n",
    "                attention.append([1] * len(batchData[i][0]))\n",
    "                continue\n",
    "            l = len(batchData[i][0])\n",
    "            padsize = maxlen - l\n",
    "            batchDatas.append(batchData[i][0] + [padc] * padsize)\n",
    "            padlabels.append(batchData[i][1] + [padi] * padsize)\n",
    "            attention.append([1] * l + [0] * padsize)\n",
    "        return batchDatas, attention, padlabels, maxlen"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "BatchSize = 100\n",
    "trainDataSet = MyDataset(Mydata['trainset']['data'], Mydata['trainset']['label'], Mydata['word2id'], Mydata['id2label'],\n",
    "                         Mydata['label2id'],device)\n",
    "\n",
    "trainDataloader = DataLoader(dataset=trainDataSet, batch_size=BatchSize,\n",
    "                             shuffle=False, collate_fn=trainDataSet.batchfy)\n",
    "\n",
    "testDataSet = MyDataset(Mydata['testset']['data'], Mydata['testset']['label'],\n",
    "                        Mydata['word2id'], Mydata['id2label'], Mydata['label2id'],device)\n",
    "testDataloader = DataLoader(dataset=testDataSet, batch_size=BatchSize,\n",
    "                            shuffle=False, collate_fn=testDataSet.batchfy)\n",
    "\n",
    "devDataset = MyDataset(Mydata['devset']['data'], Mydata['devset']['label'],\n",
    "                        Mydata['word2id'], Mydata['id2label'], Mydata['label2id'],device)\n",
    "devDataloader = DataLoader(dataset=devDataset, batch_size=BatchSize,\n",
    "                            shuffle=False, collate_fn=devDataset.batchfy)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 2.2.3 Model Definition"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class MyLSTM(nn.Module):\n",
    "    def __init__(self, hidden_size,vocalsize, tagsize,\n",
    "                 embedding_size,dropout_rate,batch_len, device,encoderon=True):\n",
    "        super(MyLSTM, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocalsize = vocalsize\n",
    "        self.tagsize = tagsize\n",
    "        self.embedding_size = embedding_size\n",
    "        self.device = device\n",
    "        self.batch_len = batch_len\n",
    "        self.encoderon=encoderon\n",
    "        if self.encoderon:\n",
    "            self.encoder = nn.Embedding(self.vocalsize, self.embedding_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding_size,\n",
    "                            hidden_size=self.hidden_size,\n",
    "                            num_layers=1,bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.output = nn.Linear(self.hidden_size*2, self.tagsize)\n",
    "\n",
    "        # self.hidden = (torch.zeros(1,2,self.hidden_size).to(self.device))\n",
    "        # self.hidden = (torch.zeros(2, self.input_size, self.hidden_size).to(self.device),\n",
    "        #        torch.zeros(2, self.input_size, self.hidden_size).to(self.device))\n",
    "        self.hidden = (torch.zeros(2, self.batch_len, self.hidden_size).to(self.device),\n",
    "               torch.zeros(2, self.batch_len, self.hidden_size).to(self.device))\n",
    "\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        l = len(sentence)\n",
    "        self.adjustHiddenSize(l)\n",
    "\n",
    "        if self.encoderon:\n",
    "            emb = self.encoder(sentence)\n",
    "        else:\n",
    "            emb = sentence\n",
    "\n",
    "        lstm_out, self.hidden = self.lstm(emb, self.hidden)\n",
    "        # lstm_out, _ = self.lstm(emb)\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "\n",
    "        output = self.output(lstm_out)\n",
    "        return output\n",
    "\n",
    "    def adjustHiddenSize(self,senlen):\n",
    "        self.hidden = (torch.zeros(2, senlen, self.hidden_size).to(self.device),\n",
    "               torch.zeros(2, senlen, self.hidden_size).to(self.device))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 2.2.3.1 Test Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 47])\n",
      "torch.Size([100, 47, 9])\n",
      "label torch.Size([100, 47])\n",
      "tensor(217.3495, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    model = MyLSTM(embedding_size=10, hidden_size=10,\n",
    "               vocalsize=len(Mydata['word2id']),\n",
    "               tagsize=len(Mydata['label2id'])-1,\n",
    "               dropout_rate=0.1,\n",
    "               batch_len=BatchSize,\n",
    "               device=device,\n",
    "               encoderon=True)\n",
    "    data,label,size = None,None,0\n",
    "    for i, (batch, atten, labe, size) in enumerate(trainDataloader):\n",
    "        data = batch\n",
    "        label = labe\n",
    "        break\n",
    "    model.to(device=device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=Mydata['label2id'][\"PAD\"])\n",
    "    with torch.no_grad():\n",
    "        inp = torch.tensor(np.matrix(data),dtype=torch.long).to(device)\n",
    "        print(inp.shape)\n",
    "        tag_scores = model(inp)\n",
    "        print(tag_scores.shape)\n",
    "        label = torch.tensor(label,dtype=torch.long).to(device)\n",
    "        print(\"label\",label.shape)\n",
    "        loss = 0.\n",
    "        for i in range(BatchSize):\n",
    "            loss += criterion(tag_scores[i],label[i])\n",
    "        print(loss)\n",
    "test()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 2.2.4 Training and Evaluating"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "model = MyLSTM(embedding_size=64, hidden_size=64,\n",
    "               vocalsize=len(Mydata['word2id']),\n",
    "               tagsize=len(Mydata['label2id']),\n",
    "               dropout_rate=0.1,\n",
    "               batch_len=BatchSize,\n",
    "               device=device,\n",
    "               encoderon=True).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=Mydata['label2id'][\"PAD\"])\n",
    "lr = 5  # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\n",
    "def train(model:nn.Module,epochi:int,dataloader:DataLoader,dtype):\n",
    "    model.train()  # turn on train mode\n",
    "    total_loss = 0.\n",
    "    log_interval = 50\n",
    "    start_time = time.time()\n",
    "    num_batches = len(dataloader)\n",
    "    for i, (batch, _, label, size) in enumerate(dataloader):\n",
    "        # print(i)\n",
    "        batch=torch.tensor(batch, dtype=dtype,device=device)\n",
    "        label = torch.tensor(label ,dtype=torch.long,device=device)\n",
    "        # print(batch.shape,batch[0])\n",
    "\n",
    "        output = model(batch)\n",
    "\n",
    "        # print(output.shape)\n",
    "\n",
    "        loss = 0.\n",
    "        for j in range(len(batch)):\n",
    "            loss += criterion(output[j],label[j])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            lr = scheduler.get_last_lr()[0]\n",
    "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "            cur_loss = total_loss / log_interval\n",
    "            ppl = math.exp(cur_loss)\n",
    "            print(f'| epoch {epochi:3d} | {i:5d}/{num_batches:5d} batches | '\n",
    "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def evaluate(model: nn.Module, eval_data: DataLoader,dtype,id2label:dict) -> tuple:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss = 0.\n",
    "    l = 0\n",
    "    predlabel_word = []\n",
    "    reallabel_word = []\n",
    "    with torch.no_grad():\n",
    "        for i, (batch, atten, label, size) in enumerate(eval_data):\n",
    "            l+=len(batch)\n",
    "            batch=torch.tensor(batch,dtype=dtype,device=device)\n",
    "            label = torch.tensor(label ,dtype=torch.long,device=device)\n",
    "\n",
    "            output = model(batch)\n",
    "            pred_id = torch.argmax(output.data,dim=-1)\n",
    "            for i in range(len(batch)):\n",
    "                total_loss += criterion(output[i],label[i])\n",
    "                tmp_pred = []\n",
    "                tmp_real = []\n",
    "                for j in range(size):\n",
    "                    if label[i][j] == 9:\n",
    "                        break\n",
    "                    tmp_real.append(id2label[label[i][j].item()])\n",
    "                    tmp_pred.append(id2label[pred_id[i][j].item()])\n",
    "                predlabel_word.append(tmp_pred[:])\n",
    "                reallabel_word.append(tmp_real[:])\n",
    "\n",
    "    return total_loss / l,\\\n",
    "           accuracy_score(reallabel_word,predlabel_word),\\\n",
    "           f1_score(reallabel_word,predlabel_word,zero_division=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report,accuracy_score,f1_score\n",
    "\n",
    "\n",
    "\n",
    "def generate_Report(model: nn.Module, eval_data: DataLoader,id2label:dict,dtype):\n",
    "    model.eval()\n",
    "    predlabel_word = []\n",
    "    reallabel_word = []\n",
    "    with torch.no_grad():\n",
    "        for _, (batch, _, label, size) in enumerate(eval_data):\n",
    "            batch=torch.tensor(batch,dtype=dtype,device=device)\n",
    "\n",
    "            output = model(batch)\n",
    "            pred_id = torch.argmax(output.data,dim=-1)\n",
    "            for i in range(len(batch)):\n",
    "                tmp_pred = []\n",
    "                tmp_real = []\n",
    "                for j in range(size):\n",
    "                    if label[i][j] == 9:\n",
    "                        break\n",
    "                    tmp_real.append(id2label[label[i][j]])\n",
    "                    tmp_pred.append(id2label[pred_id[i][j].item()])\n",
    "                predlabel_word.append(tmp_pred[:])\n",
    "                reallabel_word.append(tmp_real[:])\n",
    "    print(\"-----Output classification report:-----\")\n",
    "    print(\"ACC:\",accuracy_score(reallabel_word,predlabel_word),end=\" \")\n",
    "    try:\n",
    "        f1 = f1_score(reallabel_word,predlabel_word,zero_division=1)\n",
    "\n",
    "    except TypeError:\n",
    "        f1 = 'Nan'\n",
    "    print(\"f1: \",f1)\n",
    "\n",
    "    print(classification_report(reallabel_word,predlabel_word))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 2.2.5 Train the Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "best_val_loss = float('inf')\n",
    "epochs = 10\n",
    "best_model = None\n",
    "\n",
    "torch.manual_seed(100)\n",
    "writer = SummaryWriter(\"runs/LSTM\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(model,epoch,trainDataloader,torch.long)\n",
    "    val_loss,val_acc,val_f1 = evaluate(model, devDataloader, torch.long,Mydata['id2label'])\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "\n",
    "    writer.add_scalar('Loss',val_loss,epoch)\n",
    "    writer.add_scalar(\"Acc\",val_acc,epoch)\n",
    "    writer.add_scalar(\"F1\",val_acc,epoch)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        print(\"Find a better model\")\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(model)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "generate_Report(best_model,testDataloader,Mydata['id2label'],torch.long)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 2.2.6 Generate Vector by Glove"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from transformers import AutoModel,AutoTokenizer\n",
    "\n",
    "class MyDatasetWithGlove(MyDataset):\n",
    "    def __init__(self, data: list, label, word2id, id2label, label2id,\n",
    "                 device, tokenizer,GloveModel):\n",
    "        super(MyDatasetWithGlove,self).__init__(data, label, word2id, id2label, label2id, device)\n",
    "        self.Glovetokenizer = tokenizer\n",
    "        self.GloveModel = GloveModel\n",
    "\n",
    "    def checkVector(self,vector):\n",
    "        l = len(vector)\n",
    "        vector = vector.last_hidden_state\n",
    "        if l==1:\n",
    "            return vector[0].cpu().detach().numpy().tolist()\n",
    "        elif l>1:\n",
    "            t = vector[0]\n",
    "            for i in range(1,l):\n",
    "                t += vector[i]\n",
    "            return (t/l).cpu().detach().numpy().tolist()\n",
    "        else:\n",
    "            raise \"Vector: length error\"\n",
    "\n",
    "    def GenerateVector(self,sentence):\n",
    "        ret = []\n",
    "        for word in sentence:\n",
    "            token = self.Glovetokenizer.encode(word)\n",
    "            vec = self.GloveModel(token)\n",
    "            vec = self.checkVector(vec)\n",
    "            # print(len(vec))\n",
    "            ret.append(vec)\n",
    "        return ret\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        label = []\n",
    "        # print(item)\n",
    "        for i in range(len(self.data[item])):\n",
    "            label.append(self.label2id[self.label[item][i]])\n",
    "        data = self.GenerateVector(self.data[item])\n",
    "\n",
    "        return data, label\n",
    "\n",
    "    def batchfy(self, batchData):\n",
    "        # print(batchData)\n",
    "        maxlen = max([len(each[0]) for each in batchData])\n",
    "        padc, padi = self.GenerateVector([\"pad\"])[0], self.label2id[\"PAD\"]\n",
    "        batchDatas = []\n",
    "        padlabels = []\n",
    "        attention = []\n",
    "        for i in range(len(batchData)):\n",
    "            if len(batchData[i][0]) >= maxlen:\n",
    "                batchDatas.append(batchData[i][0])\n",
    "                padlabels.append(batchData[i][1][:])\n",
    "                attention.append([1] * len(batchData[i][0]))\n",
    "                continue\n",
    "            l = len(batchData[i][0])\n",
    "            padsize = maxlen - l\n",
    "            batchDatas.append(batchData[i][0] + [padc] * padsize)\n",
    "            padlabels.append(batchData[i][1] + [padi] * padsize)\n",
    "            attention.append([1] * l + [0] * padsize)\n",
    "        return batchDatas, attention, padlabels, maxlen"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "BatchSize = 100\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Iseratho/glove-wiki-gigaword-50\")\n",
    "GloveModel = AutoModel.from_pretrained(\"Iseratho/glove-wiki-gigaword-50\",\n",
    "                                                    trust_remote_code=True)\n",
    "\n",
    "trainGlove = MyDatasetWithGlove(Mydata['trainset']['data'], Mydata['trainset']['label'], Mydata['word2id'], Mydata['id2label'],\n",
    "                         Mydata['label2id'],device,tokenizer=tokenizer,GloveModel=GloveModel)\n",
    "trainGloveLoader = DataLoader(dataset=trainGlove, batch_size=BatchSize,\n",
    "                            shuffle=False, collate_fn=trainGlove.batchfy)\n",
    "\n",
    "testGlove = MyDatasetWithGlove(Mydata['testset']['data'], Mydata['testset']['label'],\n",
    "                        Mydata['word2id'], Mydata['id2label'], Mydata['label2id'],device,tokenizer=tokenizer,GloveModel=GloveModel)\n",
    "testGloveloader = DataLoader(dataset=testGlove, batch_size=BatchSize,\n",
    "                            shuffle=False, collate_fn=testGlove.batchfy)\n",
    "\n",
    "devGlove = MyDatasetWithGlove(Mydata['devset']['data'], Mydata['devset']['label'],\n",
    "                        Mydata['word2id'], Mydata['id2label'], Mydata['label2id'],device,tokenizer=tokenizer,GloveModel=GloveModel)\n",
    "devGloveloader = DataLoader(dataset=devGlove, batch_size=BatchSize,\n",
    "                            shuffle=False, collate_fn=devGlove.batchfy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 2.2.7 Train Model with Glove"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "modelWithGlove = MyLSTM(embedding_size=50, hidden_size=64,\n",
    "               vocalsize=1, # useless in this situation\n",
    "               tagsize=len(Mydata['label2id']),\n",
    "               dropout_rate=0.1,\n",
    "               device=device,\n",
    "               batch_len=BatchSize,\n",
    "               encoderon=False).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=Mydata['label2id'][\"PAD\"])\n",
    "lr = 1  # learning rate\n",
    "optimizer = torch.optim.SGD(modelWithGlove.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    50/  141 batches | lr 1.00 | ms/batch 126.46 | loss 78.33 | ppl 10404234763134928851667631239856128.00\n",
      "| epoch   1 |   100/  141 batches | lr 1.00 | ms/batch 125.45 | loss 44.66 | ppl 24973057650038095872.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 29.53s | valid loss  0.34 | valid ppl     1.41\n",
      "-----------------------------------------------------------------------------------------\n",
      "Find a better model\n",
      "| epoch   2 |    50/  141 batches | lr 0.95 | ms/batch 121.65 | loss 32.62 | ppl 146465655050137.66\n",
      "| epoch   2 |   100/  141 batches | lr 0.95 | ms/batch 124.59 | loss 29.34 | ppl 5507529384118.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 29.18s | valid loss  0.25 | valid ppl     1.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "Find a better model\n",
      "| epoch   3 |    50/  141 batches | lr 0.90 | ms/batch 121.70 | loss 23.98 | ppl 25902304544.03\n",
      "| epoch   3 |   100/  141 batches | lr 0.90 | ms/batch 128.83 | loss 22.56 | ppl 6276114964.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 30.16s | valid loss  0.20 | valid ppl     1.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "Find a better model\n",
      "| epoch   4 |    50/  141 batches | lr 0.86 | ms/batch 124.50 | loss 18.98 | ppl 174200326.30\n",
      "| epoch   4 |   100/  141 batches | lr 0.86 | ms/batch 126.98 | loss 18.38 | ppl 95633288.89\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 29.75s | valid loss  0.17 | valid ppl     1.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "Find a better model\n",
      "| epoch   5 |    50/  141 batches | lr 0.81 | ms/batch 122.24 | loss 15.67 | ppl 6410176.14\n",
      "| epoch   5 |   100/  141 batches | lr 0.81 | ms/batch 126.35 | loss 15.52 | ppl 5473523.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 29.65s | valid loss  0.15 | valid ppl     1.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "Find a better model\n",
      "| epoch   6 |    50/  141 batches | lr 0.77 | ms/batch 122.06 | loss 13.88 | ppl 1070168.81\n",
      "| epoch   6 |   100/  141 batches | lr 0.77 | ms/batch 125.73 | loss 13.78 | ppl 961881.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 29.80s | valid loss  0.15 | valid ppl     1.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "Find a better model\n",
      "| epoch   7 |    50/  141 batches | lr 0.74 | ms/batch 124.32 | loss 12.54 | ppl 278115.79\n",
      "| epoch   7 |   100/  141 batches | lr 0.74 | ms/batch 125.21 | loss 12.60 | ppl 297100.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 29.69s | valid loss  0.14 | valid ppl     1.15\n",
      "-----------------------------------------------------------------------------------------\n",
      "Find a better model\n",
      "| epoch   8 |    50/  141 batches | lr 0.70 | ms/batch 123.65 | loss 11.63 | ppl 112358.31\n",
      "| epoch   8 |   100/  141 batches | lr 0.70 | ms/batch 126.15 | loss 11.49 | ppl 97941.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 29.64s | valid loss  0.13 | valid ppl     1.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "Find a better model\n",
      "| epoch   9 |    50/  141 batches | lr 0.66 | ms/batch 122.27 | loss 10.77 | ppl 47372.86\n",
      "| epoch   9 |   100/  141 batches | lr 0.66 | ms/batch 127.03 | loss 10.84 | ppl 50770.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 29.50s | valid loss  0.13 | valid ppl     1.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "Find a better model\n",
      "| epoch  10 |    50/  141 batches | lr 0.63 | ms/batch 121.79 | loss 10.22 | ppl 27537.78\n",
      "| epoch  10 |   100/  141 batches | lr 0.63 | ms/batch 126.15 | loss 10.27 | ppl 28903.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 29.51s | valid loss  0.12 | valid ppl     1.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "Find a better model\n",
      "| epoch  11 |    50/  141 batches | lr 0.60 | ms/batch 122.09 | loss  9.50 | ppl 13345.42\n",
      "| epoch  11 |   100/  141 batches | lr 0.60 | ms/batch 126.33 | loss  9.40 | ppl 12056.22\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 29.36s | valid loss  0.12 | valid ppl     1.13\n",
      "-----------------------------------------------------------------------------------------\n",
      "Find a better model\n",
      "| epoch  12 |    50/  141 batches | lr 0.57 | ms/batch 123.99 | loss  9.17 | ppl  9594.53\n",
      "| epoch  12 |   100/  141 batches | lr 0.57 | ms/batch 125.27 | loss  9.02 | ppl  8247.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 29.54s | valid loss  0.12 | valid ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "Find a better model\n",
      "| epoch  13 |    50/  141 batches | lr 0.54 | ms/batch 122.28 | loss  8.65 | ppl  5717.72\n",
      "| epoch  13 |   100/  141 batches | lr 0.54 | ms/batch 124.68 | loss  8.70 | ppl  6017.04\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 29.58s | valid loss  0.12 | valid ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "Find a better model\n",
      "| epoch  14 |    50/  141 batches | lr 0.51 | ms/batch 122.67 | loss  8.25 | ppl  3815.71\n",
      "| epoch  14 |   100/  141 batches | lr 0.51 | ms/batch 124.77 | loss  8.30 | ppl  4029.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 29.44s | valid loss  0.11 | valid ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "Find a better model\n",
      "| epoch  15 |    50/  141 batches | lr 0.49 | ms/batch 121.64 | loss  7.88 | ppl  2631.02\n",
      "| epoch  15 |   100/  141 batches | lr 0.49 | ms/batch 125.96 | loss  7.94 | ppl  2809.66\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 29.41s | valid loss  0.11 | valid ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "Find a better model\n",
      "| epoch  16 |    50/  141 batches | lr 0.46 | ms/batch 121.07 | loss  7.52 | ppl  1853.28\n",
      "| epoch  16 |   100/  141 batches | lr 0.46 | ms/batch 125.77 | loss  7.79 | ppl  2426.38\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 29.36s | valid loss  0.11 | valid ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 |    50/  141 batches | lr 0.44 | ms/batch 121.10 | loss  7.35 | ppl  1559.97\n",
      "| epoch  17 |   100/  141 batches | lr 0.44 | ms/batch 124.77 | loss  7.43 | ppl  1681.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 29.27s | valid loss  0.11 | valid ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "Find a better model\n",
      "| epoch  18 |    50/  141 batches | lr 0.42 | ms/batch 121.64 | loss  6.88 | ppl   974.79\n",
      "| epoch  18 |   100/  141 batches | lr 0.42 | ms/batch 125.02 | loss  7.19 | ppl  1320.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 30.26s | valid loss  0.11 | valid ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "Find a better model\n",
      "| epoch  19 |    50/  141 batches | lr 0.40 | ms/batch 129.73 | loss  6.78 | ppl   880.00\n",
      "| epoch  19 |   100/  141 batches | lr 0.40 | ms/batch 127.07 | loss  6.98 | ppl  1074.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 29.97s | valid loss  0.11 | valid ppl     1.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 |    50/  141 batches | lr 0.38 | ms/batch 121.57 | loss  6.59 | ppl   730.98\n",
      "| epoch  20 |   100/  141 batches | lr 0.38 | ms/batch 123.91 | loss  6.82 | ppl   913.16\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 29.12s | valid loss  0.11 | valid ppl     1.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "Find a better model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Python\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ..\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:925.)\n",
      "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----Output classification report:-----\n",
      "ACC: 0.9614730268116722 f1:  0.8119886161508361\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.84      0.88      0.86      1668\n",
      "        MISC       0.69      0.64      0.66       702\n",
      "         ORG       0.75      0.74      0.75      1661\n",
      "         PER       0.91      0.88      0.89      1617\n",
      "\n",
      "   micro avg       0.82      0.81      0.81      5648\n",
      "   macro avg       0.80      0.78      0.79      5648\n",
      "weighted avg       0.81      0.81      0.81      5648\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "epochs = 20\n",
    "best_model = None\n",
    "\n",
    "torch.manual_seed(100)\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(\"runs/LSTM\")\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(modelWithGlove,epoch,trainGloveLoader,torch.float32)\n",
    "    val_loss,val_acc,val_f1 = evaluate(modelWithGlove, devGloveloader, torch.float32,Mydata['id2label'])\n",
    "    val_ppl = math.exp(val_loss)\n",
    "    elapsed = time.time() - epoch_start_time\n",
    "    print('-' * 89)\n",
    "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
    "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
    "    print('-' * 89)\n",
    "\n",
    "    writer.add_scalar('Loss',val_loss,epoch)\n",
    "    writer.add_scalar(\"Acc\",val_acc,epoch)\n",
    "    writer.add_scalar(\"F1\",val_f1,epoch)\n",
    "\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        print(\"Find a better model\")\n",
    "        best_val_loss = val_loss\n",
    "        best_model = copy.deepcopy(modelWithGlove)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "generate_Report(best_model,testGloveloader,Mydata['id2label'],torch.float32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### 2.2.8 save result file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "def getPred(model:nn.Module,eval_data:DataLoader,id2label:dict,dtype):\n",
    "    model.eval()\n",
    "    predlabel_word = []\n",
    "    with torch.no_grad():\n",
    "        for _, (batch, _, label, size) in enumerate(eval_data):\n",
    "            batch=torch.tensor(batch,dtype=dtype,device=device)\n",
    "\n",
    "            output = model(batch)\n",
    "            pred_id = torch.argmax(output.data,dim=-1)\n",
    "            for i in range(len(batch)):\n",
    "                tmp_pred = []\n",
    "                tmp_real = []\n",
    "                for j in range(size):\n",
    "                    if label[i][j] == 9:\n",
    "                        break\n",
    "                    tmp_real.append(id2label[label[i][j]])\n",
    "                    tmp_pred.append(id2label[pred_id[i][j].item()])\n",
    "                predlabel_word += tmp_pred[:]\n",
    "    return predlabel_word"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Python\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:692: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at  ..\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:925.)\n",
      "  self.dropout, self.training, self.bidirectional, self.batch_first)\n"
     ]
    }
   ],
   "source": [
    "predlabel=getPred(best_model,testGloveloader,Mydata['id2label'],torch.float32)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "import csv\n",
    "def writeTestFile(pred):\n",
    "    reader = csv.reader(open(\"./data/test.txt\"),delimiter=' ')\n",
    "    wri = csv.writer(open('./test_LSTM.txt','w+',newline=''),delimiter=' ')\n",
    "    i = 0\n",
    "    for line in reader:\n",
    "        if not line or line[0]==\"-DOCSTART-\":\n",
    "            wri.writerow(line)\n",
    "            continue\n",
    "        line[-1] = pred[i]\n",
    "        wri.writerow(line)\n",
    "        i+=1\n",
    "    if i!=len(predlabel)-1:\n",
    "        assert \"Number Error\"\n",
    "writeTestFile(predlabel)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
