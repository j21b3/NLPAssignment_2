{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1.Data Preparation\n",
    "\n",
    "At first, read the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def readData(filePath: str = \"\") -> list:\n",
    "    with open(filePath, 'r+') as f:\n",
    "        dic = {\"data\": [], \"label\": []}\n",
    "        tmpd = []\n",
    "        tmpl = []\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            if line == \"-DOCSTART- -X- -X- O\":\n",
    "                continue\n",
    "            if not line and not tmpd:\n",
    "                continue\n",
    "            elif not line and tmpd:\n",
    "                dic[\"data\"].append(tmpd[:])\n",
    "                dic[\"label\"].append(tmpl[:])\n",
    "                tmpd.clear()\n",
    "                tmpl.clear()\n",
    "            else:\n",
    "                word, _, _, tag = line.split(' ')\n",
    "                tmpd.append(word)\n",
    "                tmpl.append(tag)\n",
    "    return dic"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Decide padding size: choose the longest sentence in the data set\n",
    "Find the label nums"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longest sentence length: 124\n",
      "The num of labels:  9 {'I-PER', 'B-PER', 'I-MISC', 'I-ORG', 'B-MISC', 'B-LOC', 'O', 'I-LOC', 'B-ORG'}\n"
     ]
    }
   ],
   "source": [
    "testdata, traindata, devdata = readData('./data/test.txt'), readData('./data/train.txt'), readData('./data/valid.txt')\n",
    "print(\"longest sentence length:\",\n",
    "      max([len(each) for each in testdata['data'] + traindata['data'] + devdata['data']]))\n",
    "\n",
    "labelset = set()\n",
    "for word in testdata['label'] + traindata['label'] + devdata['label']:\n",
    "    for label in word:\n",
    "        labelset.add(label)\n",
    "print(\"The num of labels: \", len(labelset), labelset)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# generate word vector\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "# device = 'cpu'\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"nicoladecao/msmarco-word2vec256000-bert-base-uncased\")\n",
    "# model = AutoModel.from_pretrained(\"nicoladecao/msmarco-word2vec256000-bert-base-uncased\")\n",
    "\n",
    "# model.to(device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "t1 = {}\n",
    "t2 = {}\n",
    "for idx, each in enumerate(labelset):\n",
    "    t1[each] = idx\n",
    "    t2[idx] = each\n",
    "\n",
    "t1['PAD'] = len(t1)\n",
    "t2[len(t2)] = 'PAD'\n",
    "\n",
    "dataset = {\"label2id\": t1, \"id2label\": t2}\n",
    "\n",
    "\n",
    "# def tokenize_func(dataset, label2id, tokenizer, model=\"\", device=\"\", MAX_SEQ_Len=124):\n",
    "#     # PAD - padding, 1-startpadding,2-endpadding\n",
    "#     print(\"Padding ID:\", tokenizer.pad_token_id)\n",
    "#     MAX_SEQ_Len = MAX_SEQ_Len + 2\n",
    "#     data = dataset['data']\n",
    "#     label = dataset['label']\n",
    "#     sp = label2id[\"SP\"]\n",
    "#     ep = label2id[\"EP\"]\n",
    "#     pad = label2id[\"PAD\"]\n",
    "#     for i in range(len(data)):\n",
    "#         # print(data[i])\n",
    "#         s = \" \".join(data[i])\n",
    "#         # token = tokenizer(s, max_length=MAX_SEQ_Len, return_tensors=\"pt\", padding='max_length')\n",
    "#         # token.to(device)\n",
    "#         # data[i] = model(**token).last_hidden_state\n",
    "#         t = tokenizer.encode_plus(s, max_length=MAX_SEQ_Len, return_tensors=\"pt\", padding='max_length')\n",
    "#         # print(t)\n",
    "#         data[i] = [t['input_ids'],t['attention_mask']]\n",
    "#         label[i] = [sp] + [label2id[each] for each in label[i]] + [ep] + [pad] * (MAX_SEQ_Len - len(label[i]) - 2)\n",
    "#         # print(len(data[i]),len(label[i]))\n",
    "\n",
    "def tokenize_func(datas, label2id, word2id, MAX_SEQ_Len=124):\n",
    "    # 0 - padding\n",
    "    data = datas['data']\n",
    "    label = datas['label']\n",
    "    pad = label2id[\"PAD\"]\n",
    "    for i in range(len(data)):\n",
    "        l = len(data[i])\n",
    "        tokenlist = [word2id[\"PAD\"]] * MAX_SEQ_Len\n",
    "        for j in range(l):\n",
    "            w = data[i][j]\n",
    "            if w not in word2id:\n",
    "                word2id[w] = len(word2id)\n",
    "            tokenlist[j] = word2id[w]\n",
    "        label[i] = [label2id[each] for each in label[i]] + [pad] * (MAX_SEQ_Len - l)\n",
    "        attension_mask = [1] * l + [0] * (MAX_SEQ_Len - l)\n",
    "        data[i] = [tokenlist, attension_mask]\n",
    "\n",
    "def generate_dict(data):\n",
    "    dic = {'PAD':0}\n",
    "    for sen in data:\n",
    "        for w in sen:\n",
    "            if w not in dic:\n",
    "                dic[w]=len(dic)\n",
    "    return dic\n",
    "\n",
    "# tokenize_func(traindata, dataset['label2id'], dataset['word2id'], 124)\n",
    "# tokenize_func(devdata, dataset['label2id'], dataset['word2id'], 124)\n",
    "# tokenize_func(testdata, dataset['label2id'], dataset['word2id'], 124)\n",
    "\n",
    "dataset[\"trainset\"] = traindata\n",
    "dataset[\"devset\"] = devdata\n",
    "dataset[\"testset\"] = testdata\n",
    "dataset['word2id'] = generate_dict(traindata['data']+devdata['data']+testdata['data'])\n",
    "dataset['longestLen'] = 124\n",
    "# maybe can add word2vec later\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "write to file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import  json\n",
    "with open(\"./Mydata.json\",'w+') as f:\n",
    "    json.dump(dataset,f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# loaddata\n",
    "with open(\"./Mydata.json\",'r+') as f:\n",
    "    ret = json.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
